{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðƒðšð­ ð’ðœð¢ðžð§ðœðž ðð«ð¨ð£ðžðœð­: Marketing Ad challange\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\huge{ð€ð®ð­ð¡ð¨ð«: ððšð«ð¥ðšð \\ ðƒð¡ð®ð§ð ðšð§ðš}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#        This project will predict who will most likely click on the AD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume that you have a marketing campaign for which we spend 1000USD for acquiring one potential customer. For each customer that we target with our ad campaign and that clicks on the ad, we get an overall profit of 100USD. However, if we target a customer that ends up not clicking on the ad, we incur a net loss of 1000USD. Therefore, we can conclude that for each customer that was not targeted by the campaign and who clicks on the ad, we get an overall profit of 1100USD. How would you approach this problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing librairies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, make_scorer, fbeta_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "%matplotlib inline \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Data\n",
    "data = pd.read_csv(\"advertising_dsdj.csv\")\n",
    "#verifying loaded data\n",
    "data.head()#printing five line from the top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().any()#looking for the missing value in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking null value in the data set\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna()#removing null value from the data\n",
    "data.isnull().any()# checking is their still null value in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.duplicated().any()# checking for any duplicates data in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of duplicated records in the data sets = \",data.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#printing duplicate data\n",
    "data[data.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop_duplicates()#removing duplicate data from the data set\n",
    "data.duplicated().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract Descriptive statistics\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe something unusual in the age column wher the minimum age is -25 years and maximum age is too high 999 years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorting the age \n",
    "sorted_age = sorted(data[\"Age\"])\n",
    "index = [] #initaliging empty index\n",
    "for i in range(len(sorted_age)):\n",
    "    index.append(i)#adding the sorted age into the empty index\n",
    "    \n",
    "#Plotting sorted age against their index\n",
    "x = index\n",
    "y = sorted_age\n",
    "\n",
    "plt.figure(figsize = (10, 5))# figure size\n",
    "plt.scatter(x, y, s = 10)\n",
    "plt.axhline(y = 0, linestyle = \"--\", color = \"r\")# adding horizontal line y = 0\n",
    "plt.axhline(y = 100, linestyle = \"--\", color = \"r\")# adding horizontal line y = 100\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[(data['Age'] > 100) | (data['Age'] < 18)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the extreme age values\n",
    "data = data[(data['Age'] >= 18) & (data['Age'] < 100)]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to work with thwe feature \"Timestamp\", we will transmorm it to datetime, a format that will allow calculation on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Timestamp\"] = pd.to_datetime(data[\"Timestamp\"])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out if the daliy time spent on site is smaller or equal to the daily internet use. For this we creat a variable \n",
    "\"Delta\". Dalta = Daily Internet Usage - Daily Time Spent on Site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check out if the 'Daily TIme Spend on Site' \n",
    "# is actually smaller or equal to the Daily Internet Usage'\n",
    "\n",
    "data['delta'] = data['Daily Internet Usage'] - data['Daily Time Spent on Site']\n",
    "data.head()\n",
    "#sum(data['delta'] < 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_dalta = data['delta']<0\n",
    "invalid_dalta = data[invalid_dalta]\n",
    "invalid_dalta\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing rows with a delta smaller than zero\n",
    "data = data[data['delta'] >= 0]\n",
    "\n",
    "# I'll remove the column that I just created, but you could definitely keep it :-) \n",
    "data = data.drop('delta', axis=1)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive Figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cross correlation  plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot heatmap of all data with correlation coefficients visible\n",
    "data_corr = data.corr()\n",
    "plt.subplots(figsize=(10,5))\n",
    "sns.heatmap(data_corr, cmap='YlGnBu', linewidth=.005, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribution Plots With Respect To Our Target Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsetting by clicks\n",
    "click = data[\"Clicked on Ad\"] == 1\n",
    "no_click = data[\"Clicked on Ad\"] == 0\n",
    "\n",
    "# Creating the plots\n",
    "features = [\"Daily Internet Usage\", \"Daily Time Spent on Site\", \"Area Income\", \"Age\"]\n",
    "labels = [\"Click\", \"No Click\"]\n",
    "\n",
    "def creating_plots(features, labels):\n",
    "    plt.figure(figsize=(15,7))\n",
    "    for feature in features:\n",
    "        plt.subplot(2, 2, features.index(feature)+1)\n",
    "        sns.distplot(data[feature][click], label=labels[0], color=\"r\")\n",
    "        sns.distplot(data[feature][no_click], label=labels[1], color=\"b\")\n",
    "        plt.axvline(data[feature][click].mean(), linestyle = '--', color=\"r\")\n",
    "        plt.axvline(data[feature][no_click].mean(), linestyle = '--', color=\"b\")\n",
    "        plt.legend()\n",
    "\n",
    "creating_plots(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_country = pd.crosstab(index=data[\"Country\"],     \n",
    "                      columns=\"count\")  \n",
    "table_country= table_country= table_country.sort_values([\"count\"], ascending=False)\n",
    "table_country.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Max. user\", table_country.max())\n",
    "print(\"Min. user\", table_country.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the users are fairly spread out acroos the globe; the the maximum users from \n",
    "one country reaches only 9, and that are 237 different countries to consider. Therefore it \n",
    "is not appropriate \n",
    "to look at the city distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions that we will use later on\n",
    "def data_info(data):\n",
    "    return data.info()\n",
    "\n",
    "def creating_boxplots(size, target, features, data):\n",
    "    plt.figure(figsize=size)\n",
    "    for i in range(len(numerical_vars)):\n",
    "        plt.subplot(5, 3, i+1)\n",
    "        sns.boxplot(x=target, y=features[i], data=data )\n",
    "        \n",
    "def creating_distplot(size, data, features):\n",
    "    plt.figure(figsize=size)\n",
    "    for i in range(len(features)):\n",
    "        plt.subplot(5, 3, i+1)\n",
    "        sns.distplot(data[features[i]])\n",
    "        \n",
    "def crossCorrelation(data):\n",
    "    corr = data.corr()\n",
    "    plt.figure(figsize=(15,8))\n",
    "    sns.heatmap(corr, \n",
    "          xticklabels=corr.columns.values,\n",
    "          yticklabels=corr.columns.values)\n",
    "    print(corr)\n",
    "\n",
    "    \n",
    "num_vars = data.select_dtypes(exclude=['object', 'datetime64']).columns\n",
    "\n",
    "# Creating distribution plots\n",
    "creating_distplot((16,18), data, num_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the overall distributions, we can see interesting \n",
    "bimodal distributions for the \"daily internet usage\", and \"daily \n",
    "time spent on site\" which could suggest the presence of more than one group. \n",
    "Furthermore, the \"Age\" and \"area income\" are little bit skewed to the right and left respectively which could\n",
    "potentially bias\n",
    "our model later on. In a subsequent step, we have to apply a log transformation at least on the age feature.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before performing machine learning, we will transform some features to avoid\n",
    "introducing some bias due to the overall distribution of the latter. As mentioned previously,\n",
    "we have some skewed distributions and they can make our models underperformed. To potentially \n",
    "solve this issue, we will apply a logarithmic transformation to reduce the effect of outliers \n",
    "and reduce the distribution's range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our previous EDA, we observe that \"Age\" was right skewed; \n",
    "therefore this transformation will be applied to this feature.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"log_age\"] = np.log(data[\"Age\"])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Some Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "New_Data= data.drop([\"Ad Topic Line\", \"Timestamp\", \"City\"], axis=1)\n",
    "New_Data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization Of Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining categorical, numerical, and datetime variables that we will use later\n",
    "categorical_vars = [\"Ad Topic Line\", \"City\", \"Country\"]\n",
    "numerical_vars = [\"Daily Time Spent on Site\", \"Area Income\", \"Daily Internet Usage\", \"Male\", \"log_age\"]\n",
    "datetime_vars = \"Timestamp\"\n",
    "target = \"Clicked on Ad\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a scaler, then apply it to the features\n",
    "scaler = MinMaxScaler() # default=(0, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the MinMax transformation of the numerical variables\n",
    "data = pd.DataFrame(data = New_Data)\n",
    "data[numerical_vars] = scaler.fit_transform(data[numerical_vars])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Categorical Feature: One Hot-Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, here is a one-hot encoding function to properly encode the- \n",
    "#categorical features for machine learning algorithm. \n",
    "# This function is fed with the entire dataset\n",
    "final_data = pd.get_dummies(data)\n",
    "final_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
